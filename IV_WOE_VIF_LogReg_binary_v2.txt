Outliers Identification & Treatment)
**(Window1)
from sklearn.tree import DecisionTreeClassifier
# List of continuous variables
continuous_vars = ['cont_var1', 'cont_var2', 'cont_var3', 'cont_var4', 'cont_var5', 'cont_var6', 'cont_var7', 'cont_var8'] 

# Function to identify and treat outliers using 5th and 95th percentiles
def treat_outliers_mod(df, variable):
    Q1_before = df[variable].quantile(0.25)
    Q3_before = df[variable].quantile(0.75)
    IQR_before = Q3_before - Q1_before
    
    Q1_after = df[variable].quantile(0.05)
    Q3_after = df[variable].quantile(0.95)
    IQR_after = Q3_after - Q1_after
    
    lower_bound_before = Q1_before - 1.5 * IQR_before
    upper_bound_before = Q3_before + 1.5 * IQR_before
    
    lower_bound_after = Q1_after - 1.5 * IQR_after
    upper_bound_after = Q3_after + 1.5 * IQR_after
    
    # Print statistics before and after treatment
    print(f"Variable: {variable}")
    print(f"Before treatment - Q1: {Q1_before}, Q3: {Q3_before}, IQR: {IQR_before}, Lower Bound: {lower_bound_before}, Upper Bound: {upper_bound_before}")
    
    # Treat outliers
    df[variable] = np.where(df[variable] < lower_bound_after, lower_bound_after, df[variable])
    df[variable] = np.where(df[variable] > upper_bound_after, upper_bound_after, df[variable])
    
    # Print statistics after treatment
    Q1_after = df[variable].quantile(0.05)
    Q3_after = df[variable].quantile(0.95)
    IQR_after = Q3_after - Q1_after
    print(f"After treatment - Q1: {Q1_after}, Q3: {Q3_after}, IQR: {IQR_after}")

    return df

# Iterate through each continuous variable and apply outlier treatment
for var in continuous_vars:
    df = treat_outliers_mod(df, var)
    print("\n")

# After this loop, your dataframe 'df' will contain the outliers treated according to the specified method.
****************************************************************************************************************************************************
# List of continuous variables
continuous_vars = ['var1', 'var2', 'var3', 'var4']  # Adjust according to your dataset

# Function to identify and treat outliers using 5th and 95th percentiles
def treat_outliers_mod(df, variable):
    Q1 = df[variable].quantile(0.05)
    Q3 = df[variable].quantile(0.95)
    IQR = Q3 - Q1
    
    # Calculate lower and upper bounds based on IQR
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Check if there are outliers based on the bounds
    if df[variable].min() < lower_bound or df[variable].max() > upper_bound:
        # Apply outlier treatment
        df[variable] = np.where(df[variable] < lower_bound, lower_bound, df[variable])
        df[variable] = np.where(df[variable] > upper_bound, upper_bound, df[variable])
        print(f"Outlier treatment applied to {variable}")
    else:
        print(f"No outliers found in {variable}, skipping treatment")
    
    return df

# Apply outlier treatment only to variables with outliers
for var in continuous_vars:
    df = treat_outliers_mod(df, var)

# Print the dataset after outlier treatment
print("\nDataset after Outlier Treatment:")
print(df)

****************************************************************************************************

# Function to identify and treat outliers using 5th and 95th percentiles
def treat_outliers_mod(df, variable):
    Q1 = df[variable].quantile(0.05)
    Q3 = df[variable].quantile(0.95)
    IQR = Q3 - Q1
    
    # Calculate lower and upper bounds based on IQR
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Check if there are outliers based on the bounds
    if df[variable].min() < lower_bound:
        df[variable] = np.where(df[variable] < lower_bound, lower_bound, df[variable])
        print(f"Outliers detected on the lower end of {variable}, applying treatment")
    elif df[variable].max() > upper_bound:
        df[variable] = np.where(df[variable] > upper_bound, upper_bound, df[variable])
        print(f"Outliers detected on the upper end of {variable}, applying treatment")
    else:
        print(f"No outliers found in {variable}, skipping treatment")
    
    return df

# Apply outlier treatment to each variable
for var in df.columns:
    df = treat_outliers_mod(df, var)

# Print the dataset after outlier treatment
print("\nDataset after Outlier Treatment:")
print(df)

1. Optimal Binning Using Decision Tree

from sklearn.tree import DecisionTreeClassifier

def optimal_binning(df, variable, target, max_leaf_nodes=10):
    tree_model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
    tree_model.fit(df[[variable]], df[target])
    df[f'{variable}_binned'] = tree_model.apply(df[[variable]])
    return df

2.  Calculate WOE and IV

import numpy as np

def calculate_woe_iv(df, variable, target):
    lst = []
    for val in sorted(df[variable].unique()):
        total = len(df[df[variable] == val])
        positive = len(df[(df[variable] == val) & (df[target] == 1)])
        negative = len(df[(df[variable] == val) & (df[target] == 0)])
        
        if positive == 0:
            positive = 0.5
        if negative == 0:
            negative = 0.5
        
        ratio = positive / negative
        woe = np.log(ratio)
        iv = (positive - negative) * woe / len(df)
        
        lst.append({'Value': val, 'Total': total, 'Positive': positive, 'Negative': negative, 'WoE': woe, 'IV': iv})
    
    woe_df = pd.DataFrame(lst)
    iv_total = woe_df['IV'].sum()
    return woe_df, iv_total

def transform_woe(df, variable, woe_df):
    woe_dict = woe_df.set_index('Value')['WoE'].to_dict()
    df[f'{variable}_woe'] = df[variable].map(woe_dict)
    return df


3. Encode Categorical Variables

from sklearn.preprocessing import OneHotEncoder

def encode_categorical(df, categorical_vars):
    df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=True)
    return df_encoded

4. Logistic Regression

from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm

# List of continuous and categorical variables
continuous_vars = ['cont_var1', 'cont_var2', 'cont_var3']  # replace with your continuous variable names
categorical_vars = ['cat_var1', 'cat_var2']  # replace with your categorical variable names
target = 'binary_target'  # replace with your target variable name

# Dictionary to store WOE and IV values
woe_iv_dict = {}

# Create optimal bins for continuous variables, calculate WOE, and transform the variables
for var in continuous_vars:
    df = optimal_binning(df, var, target)
    woe_df, iv_total = calculate_woe_iv(df, f'{var}_binned', target)
    df = transform_woe(df, f'{var}_binned', woe_df)
    woe_iv_dict[var] = {'WOE': woe_df, 'IV': iv_total}

# Encode categorical variables
df_encoded = encode_categorical(df, categorical_vars)

# Prepare the dataset for logistic regression
woe_vars = [f'{var}_binned_woe' for var in continuous_vars]
X = df_encoded[woe_vars + list(df_encoded.columns.difference(woe_vars + [target]))]
y = df_encoded[target]

# Fit the logistic regression model using statsmodels to get a summary
X = sm.add_constant(X)
log_reg = sm.Logit(y, X).fit()

# Print the WOE and IV for each continuous variable
for var in woe_iv_dict:
    print(f"Variable: {var}")
    print("WOE Table:")
    print(woe_iv_dict[var]['WOE'])
    print(f"IV: {woe_iv_dict[var]['IV']}")
    print("\n")

# Print the summary table of the logistic regression model
print(log_reg.summary())
