Interpreting Entropy: Higher entropy indicates more uncertainty or diversity in the credit scores. Lower entropy suggests more predictability and less diversity.

**********************************************************

import pandas as pd
import numpy as np
from scipy.stats import entropy
import matplotlib.pyplot as plt

# Step 1: Load the dataset
# Assuming the dataset is in a CSV file named 'leaf.csv'
leaf = pd.read_csv('leaf.csv')

# Step 2: Extract the credit score data from the 'DNB_SBSS' column
credit_scores = leaf['DNB_SBSS'].dropna()  # Drop missing values if any

# Step 3: Discretize the credit scores into bins
# Define bins within the range 85 to 300
bins = np.linspace(85, 300, 12)  # 12 bins between 85 and 300

# Create histogram
hist, bin_edges = np.histogram(credit_scores, bins=bins)

# Step 4: Calculate entropy
# Normalize the histogram to get a probability distribution
prob_distribution = hist / hist.sum()
credit_score_entropy = entropy(prob_distribution, base=2)

print(f'Entropy of Credit Scores: {credit_score_entropy:.4f}')

# Step 5: Visualize the distribution of credit scores
plt.hist(credit_scores, bins=bins, edgecolor='k', alpha=0.7)
plt.title('Distribution of Credit Scores')
plt.xlabel('Credit Score')
plt.ylabel('Frequency')
plt.show()
